{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "data_dir='./data/aclImdb/'\n",
    "trainX,testX, trainy,testy=[],[],[],[]\n",
    "for data_type in ['train','test']:\n",
    "    for sentiment in ['pos','neg']:\n",
    "        path=os.path.join(data_dir,data_type,sentiment,'*.txt')\n",
    "        files=glob.glob(path)\n",
    "        \n",
    "        for f in files:\n",
    "            with open(f,encoding='utf8') as review:\n",
    "                if(data_type=='train'):\n",
    "                    trainX.append(review.read())\n",
    "                    trainy.append(1 if sentiment=='pos' else 0)\n",
    "                else:\n",
    "                    testX.append(review.read())\n",
    "                    testy.append(1 if sentiment=='pos' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data of size 25000\n",
      "Testing data of size 25000\n"
     ]
    }
   ],
   "source": [
    "print('Training data of size {}'.format(len(trainX)))\n",
    "print('Testing data of size {}'.format(len(testX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "trainX,trainy=shuffle(trainX,trainy)\n",
    "testX,testy=shuffle(testX,testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data of size 25000\n",
      "Testing data of size 25000\n"
     ]
    }
   ],
   "source": [
    "print('Training data of size {}'.format(len(trainX)))\n",
    "print('Testing data of size {}'.format(len(testX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def review_to_words(review):\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from cache\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir='./cache'\n",
    "cache_file='preprocessed_data.pkl'\n",
    "\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "cache_data=None\n",
    "\n",
    "try:\n",
    "    with open(os.path.join(cache_dir,cache_file),'rb') as f:\n",
    "        cache_data=pickle.load(f)\n",
    "    print('Read data from cache')\n",
    "except:\n",
    "    print('Have to preprocess the data')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cache_data is None:\n",
    "    train_data=[review_to_words(review) for review in trainX]\n",
    "    test_data=[review_to_words(review) for review in testX]\n",
    "    \n",
    "    train_label=trainy\n",
    "    test_label=trainy\n",
    "    \n",
    "    trainX=testX=trainy=testy=None\n",
    "    \n",
    "    cache_data=dict(train_data=train_data,\n",
    "                   test_data=test_data,\n",
    "                   train_label=train_label,\n",
    "                   test_label=test_label)\n",
    "    \n",
    "    with open(os.path.join(cache_dir,cache_file),'wb') as f:\n",
    "        pickle.dump(cache_data,f)\n",
    "    \n",
    "else:\n",
    "    train_data=cache_data['train_data']\n",
    "    test_data=cache_data['test_data']\n",
    "    train_label=cache_data['train_label']\n",
    "    test_label=cache_data['test_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "vocab_size=20000\n",
    "flattened_data=[y for x in train_data for y in x]\n",
    "\n",
    "word_count=Counter(flattened_data)\n",
    "\n",
    "sorted_words=[word for word, _ in word_count.most_common()]\n",
    "\n",
    "word_dict={}\n",
    "\n",
    "for idx, data in enumerate(sorted_words[:vocab_size-2]):\n",
    "    word_dict[data]=idx+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def convert_and_pad_data(word_dict,sentence,padding=500):\n",
    "    working_sentence=np.zeros(padding)\n",
    "    \n",
    "    for idx, word in enumerate(sentence[:padding]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[idx]=word_dict[word]\n",
    "        else:\n",
    "            working_sentence[idx]=1\n",
    "    return np.array(working_sentence)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "for idx,sentence in enumerate(train_data):\n",
    "    train_data[idx]=convert_and_pad_data(word_dict,train_data[idx],maxlen)\n",
    "\n",
    "\n",
    "for idx,sentence in enumerate(test_data):\n",
    "    test_data[idx]=convert_and_pad_data(word_dict,test_data[idx],maxlen)\n",
    "\n",
    "train_data=pd.DataFrame(train_data)\n",
    "test_data=pd.DataFrame(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense ,LSTM ,Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\deban\\miniconda3\\envs\\pytorchdl\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\deban\\miniconda3\\envs\\pytorchdl\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "WARNING:tensorflow:From C:\\Users\\deban\\miniconda3\\envs\\pytorchdl\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 113s 5ms/sample - loss: 0.5423 - acc: 0.7460 - val_loss: 1.0232 - val_acc: 0.5032\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 112s 4ms/sample - loss: 0.3757 - acc: 0.8501 - val_loss: 1.0494 - val_acc: 0.5039\n",
      "Epoch 3/15\n",
      "23840/25000 [===========================>..] - ETA: 4s - loss: 0.2694 - acc: 0.9013"
     ]
    }
   ],
   "source": [
    "model.fit(train_data, train_label,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(test_data, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('kerasimdb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
