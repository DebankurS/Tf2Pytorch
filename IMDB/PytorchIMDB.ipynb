{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "data_dir='./data/aclImdb/'\n",
    "trainX,testX, trainy,testy=[],[],[],[]\n",
    "for data_type in ['train','test']:\n",
    "    for sentiment in ['pos','neg']:\n",
    "        path=os.path.join(data_dir,data_type,sentiment,'*.txt')\n",
    "        files=glob.glob(path)\n",
    "        \n",
    "        for f in files:\n",
    "            with open(f,encoding='utf8') as review:\n",
    "                if(data_type=='train'):\n",
    "                    trainX.append(review.read())\n",
    "                    trainy.append(1 if sentiment=='pos' else 0)\n",
    "                else:\n",
    "                    testX.append(review.read())\n",
    "                    testy.append(1 if sentiment=='pos' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data of size 25000\n",
      "Testing data of size 25000\n"
     ]
    }
   ],
   "source": [
    "print('Training data of size {}'.format(len(trainX)))\n",
    "print('Testing data of size {}'.format(len(testX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "trainX,trainy=shuffle(trainX,trainy)\n",
    "testX,testy=shuffle(testX,testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data of size 25000\n",
      "Testing data of size 25000\n"
     ]
    }
   ],
   "source": [
    "print('Training data of size {}'.format(len(trainX)))\n",
    "print('Testing data of size {}'.format(len(testX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def review_to_words(review):\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from cache\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir='./cache'\n",
    "cache_file='preprocessed_data.pkl'\n",
    "\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "cache_data=None\n",
    "\n",
    "try:\n",
    "    with open(os.path.join(cache_dir,cache_file),'rb') as f:\n",
    "        cache_data=pickle.load(f)\n",
    "    print('Read data from cache')\n",
    "except:\n",
    "    print('Have to preprocess the data')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cache_data is None:\n",
    "    train_data=[review_to_words(review) for review in trainX]\n",
    "    test_data=[review_to_words(review) for review in testX]\n",
    "    \n",
    "    train_label=trainy\n",
    "    test_label=trainy\n",
    "    \n",
    "    trainX=testX=trainy=testy=None\n",
    "    \n",
    "    cache_data=dict(train_data=train_data,\n",
    "                   test_data=test_data,\n",
    "                   train_label=train_label,\n",
    "                   test_label=test_label)\n",
    "    \n",
    "    with open(os.path.join(cache_dir,cache_file),'wb') as f:\n",
    "        pickle.dump(cache_data,f)\n",
    "    \n",
    "else:\n",
    "    train_data=cache_data['train_data']\n",
    "    test_data=cache_data['test_data']\n",
    "    train_label=cache_data['train_label']\n",
    "    test_label=cache_data['test_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "vocab_size=5000\n",
    "flattened_data=[y for x in train_data for y in x]\n",
    "\n",
    "word_count=Counter(flattened_data)\n",
    "\n",
    "sorted_words=[word for word, _ in word_count.most_common()]\n",
    "\n",
    "word_dict={}\n",
    "\n",
    "for idx, data in enumerate(sorted_words[:vocab_size-2]):\n",
    "    word_dict[data]=idx+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def convert_and_pad_data(word_dict,sentence,padding=500):\n",
    "    working_sentence=np.zeros(padding)\n",
    "    \n",
    "    for idx, word in enumerate(sentence[:padding]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[idx]=word_dict[word]\n",
    "        else:\n",
    "            working_sentence[idx]=1\n",
    "    return np.array(working_sentence)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "for idx,sentence in enumerate(train_data):\n",
    "    train_data[idx]=convert_and_pad_data(word_dict,train_data[idx])\n",
    "\n",
    "\n",
    "for idx,sentence in enumerate(test_data):\n",
    "    test_data[idx]=convert_and_pad_data(word_dict,test_data[idx])\n",
    "\n",
    "train_data=pd.DataFrame(train_data)\n",
    "test_data=pd.DataFrame(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=5000\n",
    "embedding_dim=32\n",
    "hidden_dim=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "model=nn.Sequential(nn.Embedding(vocab_size, embedding_dim, padding_idx=0),\n",
    "                    nn.LSTM(embedding_dim, hidden_dim),\n",
    "                    nn.Linear(in_features=hidden_dim, out_features=1),\n",
    "                    nn.Sigmoid()\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
